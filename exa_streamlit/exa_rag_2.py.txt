import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["STREAMLIT_SERVER_ENABLE_FILE_WATCHER"] = "false"

import re
import hashlib
import streamlit as st
import faiss
import shutil
import json
import numpy as np

import threading
import torch
from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoModelForCausalLM,
    TextIteratorStreamer,
    BitsAndBytesConfig,
    AutoModelForSequenceClassification,
    AutoModelForSeq2SeqLM,
)
from langchain.embeddings.base import Embeddings
from langchain.callbacks.base import BaseCallbackHandler

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ëª¨ë¸Â·ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMB_MODEL_NAME       = "jhgan/ko-sbert-nli"
RERANKER_MODEL_NAME  = "cross-encoder/ms-marco-MiniLM-L-6-v2"
CHAT_NAME            = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
DEVICE_GPU           = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DEVICE_CPU           = torch.device("cpu")  # ì˜ì–´ ì„ë² ë”©ìš© CPU
CACHE_DIR            = ".exa_bge_rerank_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë²ˆì—­ ì „ìš© ëª¨ë¸ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TRANS_MODEL_KO2EN = "Helsinki-NLP/opus-mt-ko-en"
trans_tok = AutoTokenizer.from_pretrained(TRANS_MODEL_KO2EN)
trans_model = AutoModelForSeq2SeqLM.from_pretrained(TRANS_MODEL_KO2EN).to(DEVICE_GPU)




st.set_page_config(page_title="ë§ˆì¸í¬ë˜í”„íŠ¸ RAG (í•œì˜ ë°ì´í„°)", layout="wide")
st.title("â›ï¸ ë§ˆì¸í¬ë˜í”„íŠ¸ ì•„ì´í…œ RAG ì±—ë´‡ (í•œì˜ ë°ì´í„°)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ exa_translate() ëŒ€ì‹  ì“¸ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def translate_ko2en(text: str) -> str:
    """í•œê¸€ â†’ ì˜ì–´ ì „ìš© ë²ˆì—­"""
    # ìµœëŒ€ í† í° ê¸¸ì´ì— ë§ê²Œ ìë¥´ì‹œê±°ë‚˜ padding ì˜µì…˜ ì¡°ì ˆí•˜ì„¸ìš”.
    inputs = trans_tok(text, return_tensors="pt", truncation=True, padding=True).to(DEVICE_GPU)
    with torch.no_grad():
        outputs = trans_model.generate(
            **inputs,
            max_length=512,
            num_beams=4,        # ë¹” ì„œì¹˜ ì‚¬ìš©í•´ì„œ í’ˆì§ˆ í–¥ìƒ
            early_stopping=True
        )
    return trans_tok.decode(outputs[0], skip_special_tokens=True)



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ BGE Embeddings ë˜í¼ (GPU/CPU ì§€ì›) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class BgeEmbeddings(Embeddings):
    def __init__(self, model_name=EMB_MODEL_NAME, device=DEVICE_GPU, batch_size=16, max_length=512):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        dtype = torch.float16 if device.type == 'cuda' else torch.float32
        self.model     = AutoModel.from_pretrained(model_name, torch_dtype=dtype).to(device).eval()
        self.device    = device
        self.batch_size= batch_size
        self.max_length= max_length

    @staticmethod
    def _mean_pool(last_hidden_state, attention_mask):
        mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        summed = (last_hidden_state * mask).sum(dim=1)
        counted = mask.sum(dim=1).clamp(min=1e-9)
        return summed / counted

    def _embed(self, texts):
        all_embs = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i+self.batch_size]
            encoded = self.tokenizer(
                batch, padding=True, truncation=True,
                max_length=self.max_length, return_tensors='pt'
            ).to(self.device)
            with torch.no_grad():
                out = self.model(**encoded)
            emb = self._mean_pool(out.last_hidden_state, encoded['attention_mask'])
            emb = torch.nn.functional.normalize(emb, p=2, dim=1)
            all_embs.append(emb.cpu().numpy())
        if not all_embs:
            return np.zeros((0, self.model.config.hidden_size), dtype='float32')
        return np.vstack(all_embs).astype('float32')

    def embed_documents(self, texts):
        return self._embed(texts)

    def embed_query(self, text):
        return self._embed([text])[0]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ëª¨ë¸ í†µí•© ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def load_all_models():
    # ì˜ì–´ ë¦¬ë­ì»¤ (cross-encoder)
    dt = torch.float16 if DEVICE_GPU.type=='cuda' else torch.float32
    tok_r = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME)
    mdl_r = AutoModelForSequenceClassification.from_pretrained(
        RERANKER_MODEL_NAME, torch_dtype=dt, num_labels=1
    ).to(DEVICE_GPU).eval()
    # ì±— LLM
    bnb = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type='nf4',
        bnb_4bit_compute_dtype=torch.bfloat16,
        llm_int8_enable_fp32_cpu_offload=True
    )
    tok_c = AutoTokenizer.from_pretrained(CHAT_NAME, trust_remote_code=True)
    mdl_c = AutoModelForCausalLM.from_pretrained(
        CHAT_NAME, trust_remote_code=True,
        quantization_config=bnb, device_map={'':0}
    )
    return tok_r, mdl_r, tok_c, mdl_c

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM ìŠ¤íŠ¸ë¦¬ë° í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def stream_generate(prompt: str):
    llm_device = st.session_state.mdl_chat.device
    inp = st.session_state.tok_chat(prompt, return_tensors="pt").to(llm_device)
    streamer = TextIteratorStreamer(
        st.session_state.tok_chat, skip_prompt=True, skip_special_tokens=True
    )
    thread = threading.Thread(
        target=st.session_state.mdl_chat.generate,
        kwargs={
            "input_ids": inp.input_ids,
            "attention_mask": inp.attention_mask,
            "streamer": streamer,
            "max_new_tokens": 1024,
            "do_sample": False
        },
    )
    thread.start()
    for token in streamer:
        yield token
    thread.join()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ StreamHandler ì½œë°± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs):
        self.text += token
        self.container.markdown(self.text + "â–Œ")

    def on_llm_end(self, response=None, **kwargs):
        self.container.markdown(self.text)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ëª¨ë¸ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar.status("ëª¨ë¸ ë¡œë“œ ì¤‘...", expanded=True) as status:
    try:
        status.write(f"ì„ë² ë”© ëª¨ë¸ ({EMB_MODEL_NAME}) ë¡œë“œ ì¤‘...")
        st.session_state.embedder_gpu = BgeEmbeddings(device=DEVICE_GPU)
        st.session_state.embedder_cpu = BgeEmbeddings(device=DEVICE_CPU)
        status.write("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
        status.write("Reranker ë° LLM ë¡œë“œ ì¤‘...")
        tok_reranker, mdl_reranker, tok_chat, mdl_chat = load_all_models()
        st.session_state.update({
            'tok_reranker': tok_reranker,
            'mdl_reranker': mdl_reranker,
            'tok_chat': tok_chat,
            'mdl_chat': mdl_chat,
            'models_loaded': True
        })
        status.update(label="ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!", state='complete', expanded=False)
    except Exception as e:
        status.update(label=f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}", state='error', expanded=True)
        st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‚¬ì´ë“œë°”: í•œì˜ íŒŒì¼ ì—…ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.title("ë°ì´í„° íŒŒì¼ (í•œì˜)")
    ko_file = st.file_uploader("í•œê¸€ TXT ì—…ë¡œë“œ", type=["txt"], key="ko_file")
    en_file = st.file_uploader("ì˜ì–´ TXT ì—…ë¡œë“œ", type=["txt"], key="en_file")
    if st.button("ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™”", key="clear_all"):
        for k in ['en_index','ko_blocks','en_blocks','uploaded_files_key','messages']:
            st.session_state.pop(k, None)
        st.experimental_rerun()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì—…ë¡œë“œ íŒŒì¼ ì²˜ë¦¬ & ì˜ì–´ ì¸ë±ìŠ¤ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ko_file and en_file:
    st.sidebar.info("1ï¸âƒ£ íŒŒì¼ ì½ëŠ” ì¤‘â€¦")
    ko_raw = ko_file.getvalue().decode('utf-8')
    en_raw = en_file.getvalue().decode('utf-8')

    st.sidebar.info("2ï¸âƒ£ ë¸”ë¡ ë¶„í•  ì¤‘â€¦")
    #pat = r'(?:\r?\n[ \t]*\r?\n)+'
    pat = r'(?=[ \t]*"item_name")' 
    ko_blocks = [b.strip() for b in re.split(pat, ko_raw) if b.strip()]
    en_blocks = [b.strip() for b in re.split(pat, en_raw) if b.strip()]

    key = hashlib.md5((ko_raw+en_raw).encode('utf-8')).hexdigest()
    idx_en_path = os.path.join(CACHE_DIR, f"{key}_en.faiss")
    meta_en_path = os.path.join(CACHE_DIR, f"{key}_en.meta")

    # ìºì‹œ ë¡œë“œ
    if os.path.exists(idx_en_path) and os.path.exists(meta_en_path):
        st.sidebar.info("ğŸ” ì˜ì–´ ì¸ë±ìŠ¤ ìºì‹œ ë¡œë“œ ì¤‘â€¦")
        idx_en = faiss.read_index(idx_en_path)
        en_blocks = []
        with open(meta_en_path, 'r', encoding='utf-8') as f_meta:
            for line in f_meta:
                line=line.strip()
                if not line: continue
                try: blk=json.loads(line)
                except: blk=line
                en_blocks.append(blk)
        st.sidebar.success("âœ… ì˜ì–´ ì¸ë±ìŠ¤ ìºì‹œ ë¡œë“œ ì™„ë£Œ")
    else:
        st.sidebar.info("3ï¸âƒ£ ì˜ì–´ ë¸”ë¡ ì„ë² ë”©(GPU->CPU ë°°ì¹˜) ì¤‘â€¦")
        # CPU embed
        vecs_en = st.session_state.embedder_cpu.embed_documents(en_blocks)
        idx_en = faiss.IndexFlatIP(len(vecs_en[0]))
        idx_en.add(np.array(vecs_en, dtype='float32'))
        faiss.write_index(idx_en, idx_en_path)
        with open(meta_en_path, 'w', encoding='utf-8') as mf:
            for blk in en_blocks: mf.write(json.dumps(blk, ensure_ascii=False)+"\n")
        st.sidebar.success("âœ… ì˜ì–´ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

    st.session_state.en_index = idx_en
    st.session_state.ko_blocks = ko_blocks
    st.session_state.en_blocks = en_blocks
    st.session_state.uploaded_files_key = key
    st.session_state.messages = []
    st.sidebar.info("4ï¸âƒ£ ì±„íŒ… ì…ë ¥ ëŒ€ê¸° ì¤‘â€¦")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ ì±„íŒ… UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if 'messages' not in st.session_state:
    st.session_state.messages=[]
for m in st.session_state.messages:
    with st.chat_message(m['role']): st.markdown(m['content'])

if ko_file and en_file and (query_ko := st.chat_input("ì§ˆë¬¸ì„ í•œê¸€ë¡œ ì…ë ¥í•˜ì„¸ìš”â€¦")):
    st.session_state.messages.append({'role':'user','content':query_ko})
    with st.chat_message('user'): st.markdown(query_ko)

    query_en = translate_ko2en(query_ko)
    with st.chat_message('assistant'):
        st.markdown("**1) ë²ˆì—­ëœ ì§ˆë¬¸ (ì˜ì–´)**")
        st.write(query_en)

        st.markdown("**2) ì˜ì–´ RAG í›„ë³´ (ìƒìœ„ 10)**")
        q_vec = st.session_state.embedder_cpu.embed_query(query_en).reshape(1,-1)
        if 'en_index' in st.session_state and st.session_state.en_index is not None:
            st.write(f"Debug: FAISS index ntotal: {st.session_state.en_index.ntotal}")
        else:
            st.write("Debug: FAISS index (en_index) is not initialized.")

        if 'en_blocks' in st.session_state and st.session_state.en_blocks is not None:
            st.write(f"Debug: Length of en_blocks: {len(st.session_state.en_blocks)}")
        else:
            st.write("Debug: en_blocks is not initialized or is None.")

        D,I=st.session_state.en_index.search(np.array(q_vec,dtype='float32'),k=min(20,st.session_state.en_index.ntotal))
        st.write(f"Debug: FAISS returned indices I[0]: {I[0][:5]}") # ì²˜ìŒ 25ê°œ ì¸ë±ìŠ¤ë§Œ ì¶œë ¥

        en_candidates_data = [] # ê° ìš”ì†Œ: {'text': ì˜ì–´ ì²­í¬, 'original_block_idx': en_blocksì—ì„œì˜ ì›ë³¸ ì¸ë±ìŠ¤}
        for score, original_block_idx_val in zip(D[0],I[0]):
            if 0 <= original_block_idx_val < len(st.session_state.en_blocks):
                txt = st.session_state.en_blocks[original_block_idx_val]
                # txtê°€ ì´ë¯¸ ë¬¸ìì—´ì´ê² ì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´ str() ì²˜ë¦¬ ê³ ë ¤ ê°€ëŠ¥
                en_candidates_data.append({'text': str(txt), 'original_block_idx': original_block_idx_val})
                snippet = str(txt).replace("\n"," ")[:100]+"â€¦"
                st.write(f"- score={score:.4f}, chunk={snippet} (original_idx: {original_block_idx_val})")
            else:
                st.write(f"Debug: Invalid original_block_idx_val {original_block_idx_val} skipped.")

        if not en_candidates_data:
            st.warning("ê´€ë ¨ëœ ì˜ì–´ í›„ë³´ ë¬¸ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            st.session_state.messages.append({'role': 'assistant', 'content': "ì£„ì†¡í•©ë‹ˆë‹¤, ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."})

        st.markdown("**3) ì˜ì–´ ë¦¬ë­í‚¹ ê²°ê³¼ (ìƒìœ„ 3)**")
        # pairsëŠ” [ì§ˆë¬¸, ë¬¸ì„œ] ìŒì˜ ë¦¬ìŠ¤íŠ¸
        pairs_for_reranker = [[query_en, item['text']] for item in en_candidates_data]

        if not pairs_for_reranker:
            st.warning("ë¦¬ë­í‚¹í•  í›„ë³´ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤. (pairs_for_reranker is empty)")
            ko_contexts = [] # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
            # ì´ ê²½ìš°, LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë¯€ë¡œ, ì•„ë˜ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM í˜¸ì¶œ ë¡œì§ì—ì„œ ì²˜ë¦¬ í•„ìš”
        else:
            inputs = st.session_state.tok_reranker(
                pairs_for_reranker, padding=True, truncation=True, return_tensors='pt', max_length=512
            ).to(DEVICE_GPU)
            
            with torch.no_grad():
                rerank_scores = st.session_state.mdl_reranker(**inputs).logits.view(-1).cpu().numpy()
            
            # ë¦¬ë­í‚¹ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ëœ ì¸ë±ìŠ¤ (pairs_for_rerankerì— ëŒ€í•œ ì¸ë±ìŠ¤)
            # ìƒìœ„ 3ê°œ ë˜ëŠ” ì¡´ì¬í•˜ëŠ” í›„ë³´ ìˆ˜ë§Œí¼ ì„ íƒ
            num_top_rerank = min(3, len(pairs_for_reranker))
            top_rerank_indices = np.argsort(rerank_scores)[::-1][:num_top_rerank]

            ko_contexts = []
            for rank, rerank_idx in enumerate(top_rerank_indices, 1):
                # rerank_idxëŠ” pairs_for_reranker (ê·¸ë¦¬ê³  en_candidates_data)ì— ëŒ€í•œ ì¸ë±ìŠ¤
                selected_item = en_candidates_data[rerank_idx]
                snippet = selected_item['text'].replace("\n"," ")[:100]+"â€¦"
                st.write(f"{rank}. score={rerank_scores[rerank_idx]:.4f}, chunk={snippet}")
                
                # ko_contexts ë§¤ì¹­: selected_item['original_block_idx']ë¥¼ ì‚¬ìš©
                original_ko_block_idx = selected_item['original_block_idx']
                if 0 <= original_ko_block_idx < len(st.session_state.ko_blocks):
                    ko_contexts.append(st.session_state.ko_blocks[original_ko_block_idx])
                else:
                    st.warning(f"Warning: Korean context for index {original_ko_block_idx} not found or out of bounds.")
        
        st.markdown("**4) ìµœì¢… LLM í”„ë¡¬í”„íŠ¸**")
        prompt = (
            "ë‹¹ì‹ ì€ ë§ˆì¸í¬ë˜í”„íŠ¸ ì•„ì´í…œ ì „ë¬¸ AIì…ë‹ˆë‹¤.\n"
            "ëŒ€ë‹µ ë‚´ìš©ì€ ì•„ë˜ì— ìˆëŠ” ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ëŒ€ë‹µí•˜ê³  ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ í™œìš©í•˜ì§€ë§ˆì„¸ìš”.\n"
            "[CONTEXT]\n" + "\n\n".join(ko_contexts) + "\n\n"
            "[QUESTION]\n" + query_ko + "\n"
            "[ANSWER]"
        )
        st.code(prompt, language='text')

        # ë‹µë³€ ìƒì„±
        handler = StreamHandler(st.empty())
        answer = ""
        st.write("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘â€¦")
        for tok in stream_generate(prompt): # stream_generate í•¨ìˆ˜ê°€ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
            answer += tok
            handler.on_llm_new_token(tok)
        handler.on_llm_end(None)
        st.session_state.messages.append({'role':'assistant','content':answer})
